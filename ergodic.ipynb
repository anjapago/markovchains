{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergodic Markov Chain\n",
    "\n",
    "Consider an ergodic markov chain with three states. This notebook will demonstrate how to determine the steady state equilibrium probabilities using exact equations, the \"matrix multiplication\" scheme, and based on simulations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Matrix\n",
    "\n",
    "The transition matrix for the ergodic chain with three states is $M_{ergodic}$. \n",
    "\n",
    "$$M_{ergodic} = \\begin{bmatrix} m_{11} & m_{12} & m_{13} \\\\ m_{21} & m_{22} & m_{23} \\\\ m_{31} & m_{32} & m_{33} \\end{bmatrix} = \\begin{bmatrix} 0.3 & 0.2 &0.5 \\\\ 0.4 &0.1 & 0.5 \\\\ 0.3 & 0.6 & 0.1 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "## State Occupation Probability\n",
    "\n",
    "The state occupation probability at time zero is denoted $\\Pi(0)$. This vector could be $\\Pi(0)=\\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}^T$, corresponding to starting in state $1$. This vector could also be a probability vector of the chance of starting in any of the states, for example a uniform starting probability vector: $\\Pi(0)=\\begin{bmatrix} 1/3 & 1/3 & 1/3 \\end{bmatrix}^T$. \n",
    "\n",
    "To determine the probability of transitioning to each state after one step of the chain has been made, this will be determined by the state it is currently in, defined by $\\Pi(0)$, and the transition probabilities from the current state. To calculate the state occupation probability vector at step 1, the equation is $\\Pi(1)= M_{ergodic}^T \\Pi(0)$.\n",
    "\n",
    "This can be seen easily extend to a recurisve equation that will enable calculating the state occupation probability vector at any time $n$ based on the probability vector from the previous time step $n-1$, the equation is: $\\Pi(n)= M_{ergodic}^T \\Pi(n-1)$. This can therefore be used to calculate the probability vector at any time, given the vector at time $0$ : $\\Pi(n)= (M_{ergodic}^T)^n \\Pi(0)$.\n",
    "\n",
    "As the number of time steps approaches infinity, the equation becomes: $\\Pi(\\infty)= (M_{ergodic}^T) \\Pi(\\infty)$, where the probability state vector converges to the \"stationary state\". This Stationary state probability vector can be seen as the eigenvector corresponding to eigenvalue of $\\lambda=1$ of the transition matrix.\n",
    "\n",
    "Therfore, two ways to calculate this stationary state vector are:\n",
    "\n",
    "* find the eigenvalues of the transition matrix, and the corresponding eigenvector\n",
    "* approximate the transition matrix at $(M_{ergodic}^T)^{\\infty}$ to compute the stationary state based on $\\Pi(0)$\n",
    "\n",
    "These two approaches will be calculated. Then the markov chain will be simulated and run for many iterations, and the final state counts will be used as a way to calculate the approximate stationary state distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Calculation of Equilibrium Probabilities\n",
    "\n",
    "The exact probabilities of the stationary (equilibrium) state are given by the equation $\\Pi(\\infty) = M_{ergodic}^T\\Pi(\\infty)$. Therefore, the stationary state probability vector can be interpreted as the eigenvector corresponding to the eigenvalue of the transition matrix $M_{ergodic}^T$ with the eignevalue $\\lambda = 1$. The equation for eigenvectors is $Av=\\lambda v$ where $A$ is a matrix, $v$ is a vector (the eigenvector), and $\\lambda$ is a scalar caled the eigenvalue. Therefore, to find the stationary state probability vector, the eigenvectors must be found, and the vector associated with eigenvalue of $\\lambda=1$ will be the probabilities of each state being occupied as the chain continues to run towards infinity.\n",
    "\n",
    "The computation of the eigenvectors and values is shown in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1. , -0.1, -0.4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mergodic = np.matrix([[0.3, 0.2, 0.5], [0.4, 0.1, 0.5], [0.3, 0.6, 0.1]])\n",
    "eigenvalues, eigenvectors = np.linalg.eig(Mergodic.T)\n",
    "\n",
    "# confirm that there is an eigenvalue equal to 1\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the corresponding eigenvector as the stationary state vector\n",
    "err_tol = 1e-9\n",
    "stationarystate= eigenvectors[:,np.where(abs(eigenvalues[0]-1)< err_tol)]\n",
    "stationarystate # view the stationary state vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.57270844, -0.53901971, -0.61762675]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that this is an eigenvector with eigevalue 1 : dot(M, v) = v\n",
    "np.dot(Mergodic.T, np.squeeze(np.asarray(stationarystate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.33116883,  0.31168831,  0.35714286])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationarystatenorm = np.squeeze(np.asarray(stationarystate))/sum(stationarystate).item(0)\n",
    "stationarystatenorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "\n",
    "The stationary state can also be calculate by approximating $(M_{ergodic}^T)^{\\infty}$, as will be shown in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minfty = (Mergodic.T)**64\n",
    "\n",
    "# view the results\n",
    "Minfty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stationary state will be the same regardless of the starting vector, as shown in the following code. This result also agrees with the vector previously calculated from the eigenvector approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting state = 1, stationary state= [[ 0.33116883  0.31168831  0.35714286]]\n",
      "starting state = 2, stationary state= [[ 0.33116883  0.31168831  0.35714286]]\n",
      "starting state = 3, stationary state= [[ 0.33116883  0.31168831  0.35714286]]\n",
      "stationary state from eigenvector = [ 0.33116883  0.31168831  0.35714286]\n"
     ]
    }
   ],
   "source": [
    "print(\"starting state = 1, stationary state= \" +str(np.dot(Minfty, [1,0,0])))\n",
    "print(\"starting state = 2, stationary state= \" +str(np.dot(Minfty, [0,1,0])))\n",
    "print(\"starting state = 3, stationary state= \" +str(np.dot(Minfty, [0,0,1])))\n",
    "print(\"stationary state from eigenvector = \" + str(stationarystatenorm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations\n",
    "\n",
    "The calculated results will now be compared with results from simulations. Experiments will be run with different starting states, and the chains will run until the approach steady state. The number of counts for being in each state across all the experiments will be used to calculate the stationary state distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the vector to determine the chance of being in a given state at time 0\n",
    "numstates = 3\n",
    "initdist= [1/2, 1/4, 1/4]\n",
    "\n",
    "# use this distribution to choose the starting state:\n",
    "startstate= np.random.choice(a= list(range(numstates)), p=np.squeeze(np.asarray(initdist)))+1\n",
    "startstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set number of experiments and number of iterations for the markov chain:\n",
    "numexp = 100\n",
    "numiter = 1000\n",
    "# matrix to store the state at each iteration of the markov chain\n",
    "statemat = np.zeros((numexp, numiter))\n",
    "startstates = np.zeros(numexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in range(numexp):\n",
    "    # choose start state for this experiment:\n",
    "    currstate= np.random.choice(a= list(range(numstates)), p=np.squeeze(np.asarray(initdist)))+1\n",
    "    startstates[exp] = currstate\n",
    "    # iterate around the markov chain 100 times\n",
    "    for niter in range(numiter):\n",
    "        statevec = np.zeros((numstates,1))\n",
    "        statevec[currstate-1]=1\n",
    "        statetransprobs = np.dot(Mergodic.T, statevec)\n",
    "        currstate = np.random.choice(a= list(range(numstates)), p=np.squeeze(np.asarray(statetransprobs)))+1\n",
    "        statemat[exp, niter]= currstate\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.33046,  0.31186,  0.35768])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the total number of states in the full matrix, to approximate steady state counts\n",
    "unique, counts = np.unique(statemat, return_counts=True)\n",
    "counts/np.product(statemat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting state = 1, stationary state= [[ 0.33116883  0.31168831  0.35714286]]\n",
      "starting state = 2, stationary state= [[ 0.33116883  0.31168831  0.35714286]]\n",
      "starting state = 3, stationary state= [[ 0.33116883  0.31168831  0.35714286]]\n",
      "stationary state from eigenvector = [ 0.33116883  0.31168831  0.35714286]\n",
      "stationary state from simulatons: [ 0.33046  0.31186  0.35768]\n"
     ]
    }
   ],
   "source": [
    "# confirm the final result is the same as the previous caluated values\n",
    "print(\"starting state = 1, stationary state= \" +str(np.dot(Minfty, [1,0,0])))\n",
    "print(\"starting state = 2, stationary state= \" +str(np.dot(Minfty, [0,1,0])))\n",
    "print(\"starting state = 3, stationary state= \" +str(np.dot(Minfty, [0,0,1])))\n",
    "print(\"stationary state from eigenvector = \" + str(stationarystatenorm))\n",
    "print(\"stationary state from simulatons: \" + str(counts/np.product(statemat.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.45,  0.28,  0.27])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(statemat[:,numiter-1], return_counts=True)\n",
    "counts/np.product(statemat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.29,  0.38,  0.33])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(statemat[:,1], return_counts=True)\n",
    "counts/np.product(statemat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " start state distribution is: [0.5, 0.25, 0.25]\n",
      " simulated start state distribution is: [ 0.53  0.27  0.2 ]\n"
     ]
    }
   ],
   "source": [
    "# check the start state distribution is close to the set starting state dist\n",
    "unique, counts = np.unique(startstates, return_counts=True)\n",
    "counts/np.product(statemat.shape[0])\n",
    "print(\" start state distribution is: \" +str(initdist) + \"\\n simulated start state distribution is: \" +str(counts/np.product(statemat.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
